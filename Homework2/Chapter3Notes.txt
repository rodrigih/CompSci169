Notes for Optimization
--------------------------------------------------------------
=== 3.2 Necessary and Sufficient Conditions for Optimality ===
--------------------------------------------------------------

- A point is a local minimum if all other points in neighbourhood has higher function value

- The point x* = [x1*,x2*,...,xn*]T is a weak local minimum if there exists a delta > 0 such that
  f(x*) <= f(x) for all x such that ||x-x*|| < delta. 
  
- x is a strong local minimum if f(x*) < f(x) for all x suck that ||x-x*|| < delta

- x is a gloabl minimum if f(x*) <= f(x) for all x in real numbers
	- Meaning of double bars: quantity that in some sense describes length size or extent of object
	( length of a vector)
	-x* means conjugate transpose
		- conjugate transpose
		
-Necessary Conditions for Optimality
	- if f is first order continuous, the necessary condition for x* to be local minimum is:
				gradient(derivative) f(x*) = 0
		- a point x* that satisfies this condition is called a stationary point( it can be a
		minimum of maximum, or a saddle point
		
- Sufficiency Conditions
	- the sufficient conditions for x* to be a strict local minimum are:
		gradient f(x*) = 0
		gradient^2 f(x*) is positive definite
			- A n*n Matrix is positive definite means z^T * M * z is positive for every non-zero 
			  column vector z of n real numbers.

--------------------------------------------------------------			  
=== 3.3 Convexity ===
--------------------------------------------------------------

- Convexity
	- Set in R^n is convex if for every x1,x2 in S and every real number a where  0 < a < 1
		the point ax1 + (q-a)x2 is in S.
		- In other words for every two points in the set, every point on the line segment joining
		  these two points is also in the set
		  
- Properties of convex functions
	1. Let f be C^1. Then f is convex over a set S iff
	   f(y) >= f(x) + gradient(x)^T(y-x)
	2. Let f be c^2. f is convex over a convex set S containing an interior point iff second 
	   derivative is positive semi-definite throughout S.
	3. if f is a convex function, then any relative minimum of f is a gloabl minimum.
	4. If a function f is convex and satisties gradientf(x*)=0, then x* is a global minimum of f

--------------------------------------------------------------
=== 3.4 Basic conecpts: Starting design, Direction Vector, and step size ===
--------------------------------------------------------------

- Most numerical methods of minimization has a starting point, called x0. From this point, choose
  a direction, d0, and a step size a0. From this point on, the design point is updated as
  x1 = x0 + a0 * d0. Repeat steps until minimum is found.

--------------------------------------------------------------  
 === 3.5 Steepest Descent Method ===
--------------------------------------------------------------
 
 - Direction Vector
	- xk is current point at kth iteration. k=0 means the starting point. Choose downhill direction
	d and step size a > 0 such that xk + ad is better (that is, f(kx + ad) < f(xk)). 
	- steepest descent based on choosing d at kth iteration where
	  dk = -(gradient)f(xk)
	  
- Line Search 
	- choosing how far to go(i.e. the step) is a matter of minimizing the following function
	  f(a) = f(xk + adk)

- Stopping Criteria
	1. || gradient f(x_k) || <= epsilon_G 
		where epsilon_G is a tolerance on the gradient and supplied by user
	2. | f(x_k + 1) - f(x_k)| <= epsilon_A + epsilon_R * |f(x_k)|
		where epsilon_A = absolute tolerance on change in function value and e_R = relative tolerance

- Steepest Descent Algorithm
	1. Select starting point and parameters epsilon_G, epsilon_A, epsilon_R
	   Set iteration indez k = 0
	2. Compute gradient f(x_K). 
	   Stop if stopping criteria 1.
	   Otherwise define normalized direction vector d_k = -(gradient)f(x_k) / || (gradient) f(x_k) ||
	3. Obtain a_k from line search techniques. Update x_(k+1) = x_k + a_k * d_k
	4. Evaluate f(x_(k+1)) Stop if stopping criteria 2 is satisfied for two successful iterations.
	   Otherwise set k = k + 1, x_k = x_(k+1) and go to step 2
	   
	   
--------------------------------------------------------------
=== 3.6 Conjugate Gradient Method ===
--------------------------------------------------------------

- Consider problem of minimizing a quadratic function

	q(x) = (1/2) * x^T * A * x + c^T * x
	
	where A is symmetric and positive definite.
	- We define conjugate direction as vectors that satisfy
		d^(i * T)*A*d^j = 0 ;  i != j ; 0 <= i ; i <= n

- Method of conjugate directions is as follows. 
	- Start with initial point x_0 and a set conjugate direction d^0,...,d^(n-1)
	- Minimize q(x) along d^0 to obtain x^1.
	- From x^1 minimize q(x) along d^2 to obtain x^2
	- In general, minimize q(x) along d^(n-1) to obtain x^n.
	- x^n is the minimum solution. 
	- The gradients of q are used to generate the conjugate directions
	- g is the gradient of q, with 
	  
	  g_k = gradient q(x_k) = A * x_k + c.
	  
	  where x_k is the current point with k = an iteration index.
	  
	  - the first direction, d_0 is chosen as the steepest descent direction, -g_0. We proceed to find a
	    new point by minimizing q(x) along d_k. 
		
		x_(k+1) = x_k + a_k * d_k

Algorithm

1.  Choose starting vector x_0
	set iteration k at 0
	d_0 = - gradient (x_0)
	
	find a_k by minimizing equation f(x_k + ad_k)
	
	find x_(k+1)
	find beta by using [g_(k+1) * (g_(k+1) - g_k) ]/ (g_k)^2
	where g = gradient q(x_k)
	
	find d_(k+1) using - gradient f(x_k) + beta_k * dk

	
functions should in form:

def func(var):
	return var[0] + var[1] + var[2]
start = [1,2,3] 

where var is an array containing the values for var to be plugged in.
It is up to user to manually input all individual variables (var[i] where i is the index)

Files will have the whole function in the file already
Step size and starting point will be an arbituary number when optimizing one dimension for 
finding alpha.

There should also be another file that defines all the function's derivatives, and puts them in a list
called func_deriv

An example of a file with a function, called input1.txt, is:

def func(var):
	return var[0] + var[1] + var[2]

An example of a file with the functions derivatives, called input1Derivatives.txt would be:

def deriv1:
	return 1
def deriv2:
	return 1
def deriv3:
	return 1

func_deriv = [deriv1,deriv2,deriv3]
start = [1,2,3]
	
